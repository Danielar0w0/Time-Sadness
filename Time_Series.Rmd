---
title: "Time Series"
author: "Daniela Dias, Diogo Marto"
date: "2025-05-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction

### 1.1 Contextualization and Purpose of the Work

Technology evolves rapidly, with programming languages, frameworks, and libraries experiencing varying degrees of adoption and community interest over time. Understanding these trends is crucial for developers, educators, organizations, and technology companies to make informed decisions, such as those related to skill development and resource allocation.

Stack Overflow, as the world's largest programming community platform, serves as an excellent proxy for measuring the popularity of technology and developer interest. The frequency of questions asked about specific technologies reflects real-world usage patterns, learning curves, and community engagement levels.

This study aims to analyze the temporal dynamics of technology usage patterns using time series analysis, with a specific focus on trends in the Python programming language. The primary objectives are:

1.  **Trend Analysis**: Identify and quantify long-term growth or decline patterns in Python usage
2.  **Seasonality Detection**: Discover recurring patterns that may reflect academic cycles, release schedules, or industry practices
3.  **Forecasting**: Develop reliable predictive models for future Python usage trends
4.  **Model Comparison**: Evaluate different time series modeling approaches to determine optimal forecasting strategies
5.  **Uncertainty Quantification**: Provide confidence intervals and assess forecast reliability

The practical value of this analysis extends to multiple stakeholders: - **Educational institutions** can adjust their curriculum based on technology trends - **Technology companies** can make informed decisions about product development and support - **Individual developers** can guide career development and skill acquisition - **Researchers** can understand technology adoption patterns and community dynamics

### 1.2 Dataset Description and Source

The dataset used in this analysis was obtained from Kaggle (<https://www.kaggle.com/datasets/aishu200023/stackindex>) and represents a comprehensive collection of Stack Overflow question counts for various programming technologies and libraries.

**Dataset Characteristics:** - **Source**: Stack Overflow question frequency data - **Time Period**: Monthly observations from January 2009 onwards - **Granularity**: Monthly aggregated counts - **Scope**: Multiple programming languages, frameworks, and libraries - **Structure**: Each column represents a different technology (e.g., Python, R, TensorFlow, scikit-learn) - **Observations**: Each row contains question counts for all technologies in a specific month

**Key Technologies Included:** The dataset encompasses a wide range of technologies, including: - **Programming Languages**: Python, R, MATLAB - **Machine Learning Libraries**: TensorFlow, scikit-learn, Keras, PyTorch - **Data Science Tools**: pandas, NumPy, SciPy, matplotlib - **Web Frameworks**: Various Python and other web development frameworks - **Specialized Libraries**: OpenCV, NLTK, spaCy, and many others

**Data Quality Considerations:** - Monthly aggregation smooths short-term fluctuations while preserving long-term trends - Stack Overflow's global reach provides internationally representative data - Question frequency serves as a proxy for technology usage and learning activity - The dataset reflects both professional and educational technology use patterns

### 1.3 Methodology Overview

This study employs classical time series analysis techniques following established statistical methodologies:

**1. Exploratory Data Analysis** - Time series visualization and decomposition - Stationarity testing using Augmented Dickey-Fuller (ADF) and KPSS tests - Seasonal pattern identification and quantification - Data transformation assessment using Box-Cox methodology

**2. Model Development (Box & Jenkins Methodology)** - **SARIMA Models**: Systematic identification using ACF/PACF analysis, parameter estimation, and diagnostic evaluation - **ETS Models**: Exponential smoothing with various error, trend, and seasonal components

**3. Forecast Generation and Evaluation** - Out-of-sample forecast validation using train/test split - Multiple accuracy metrics (RMSE, MAE, MAPE, MASE) - Confidence interval analysis and coverage assessment - Bootstrap methodology for non-parametric uncertainty quantification

**4. Model Comparison and Selection** - Information criteria comparison (AIC, BIC) - Forecast accuracy assessment - Diagnostic evaluation, including residual analysis - Parameter significance testing

**Focus on Python Technology Trends** While the dataset contains numerous technologies, this analysis focuses specifically on Python programming language trends due to: - Python's significant growth in the data science and machine learning domains - Strong seasonal patterns related to academic and professional cycles - Sufficient data history for robust statistical analysis - High relevance to the current technology landscape

This methodological approach ensures rigorous statistical analysis while providing practical insights for understanding and forecasting technology adoption patterns in the programming community.

## 2. Data and exploratory analysis / data transformation

```{r libraries}
library(tidyverse)
library(forecast)
library(tseries)
library(ggplot2)
library(lubridate)
library(gridExtra)
library(reshape2)
```

### 2.1 Data Loading and Initial Exploration

```{r data-loading}
# Load the data
data <- read.csv("data.csv")

# Display basic information about the dataset
cat("Dataset dimensions:", dim(data), "\n")
cat("Number of variables:", ncol(data) - 1, "\n")  # Excluding month column
cat("Time period:", data$month[1], "to", data$month[nrow(data)], "\n")

# Show first few rows
head(data[, 1:10])  # Show first 10 columns to avoid overwhelming output
```

### 2.2 Data Preprocessing

```{r data-preprocessing}
# Convert month column to proper date format
data$date <- as.Date(paste0("01-", data$month), format = "%d-%y-%b")

# Remove any rows with NA dates
data <- data[!is.na(data$date),]

# Sort by date
data <- data[order(data$date),]

# Print date range for verification
cat("Date range:", as.character(range(data$date)), "\n")

# Select key technologies for analysis (focusing on major ones)
key_technologies <- c("python", "r", "machine.learning", "tensorflow",
                      "deep.learning", "time.series", "scikit.learn")

# Create a subset for focused analysis
tech_data <- data[, c("date", key_technologies)]

# Display summary statistics
summary(tech_data[, -1])
```

### 2.3 Time Series Visualization

```{r time-series-plots, fig.width=12, fig.height=10}
# Create time series plots for key technologies
plots <- list()

for (i in 2:ncol(tech_data)) {
  tech_name <- names(tech_data)[i]

  p <- ggplot(tech_data, aes(x = date, y = .data[[tech_name]])) +
    geom_line(color = "steelblue", size = 0.8) +
    geom_smooth(method = "loess", se = TRUE, alpha = 0.3) +
    labs(title = paste("Time Series:", str_replace_all(tech_name, "\\.", "-")),
         x = "Date", y = "Count") +
    theme_minimal() +
    theme(plot.title = element_text(size = 10))

  plots[[tech_name]] <- p
}

# Arrange plots in a grid
if (length(plots) > 0) {
  do.call(grid.arrange, c(plots, ncol = 2))
} else {
  cat("No plots to display - check data processing\n")
}

# Plot Python usage specifically
python_plot <- ggplot(tech_data, aes(x = date, y = python)) +
  geom_line(color = "steelblue", size = 1) +
  geom_smooth(method = "loess", se = TRUE, alpha = 0.3) +
  labs(title = "Python Usage Over Time",
       x = "Date", y = "Count") +
  theme_minimal()
print(python_plot)
```
### 2.4 Box-Cox

```{r Box-Cox}
lambda_bc <- BoxCox.lambda(python_ts)
cat("Optimal Box-Cox lambda:", lambda_bc, "\n")

# Apply Box-Cox transformation using the estimated lambda.
# We apply it only if the lambda suggests a significant transformation
# (i.e., not very close to 1). Small deviations might not warrant a transform.
# For demonstration, we will apply it if lambda is not exactly 1 or 0.
# If lambda is 0, BoxCox() automatically applies log transformation.
if (abs(lambda_bc - 1) > 0.01) { # Apply if lambda is not very close to 1
  python_ts_transformed <- BoxCox(python_ts, lambda = lambda_bc)
  cat("Box-Cox transformation applied using lambda:", lambda_bc, "\n")
  # Plot the transformed series to visually inspect variance stabilization
  plot(python_ts_transformed, main = "Python Usage Over Time (Box-Cox Transformed)",
       ylab = paste0("Transformed Count (lambda = ", round(lambda_bc, 2), ")"))
  
  # Update python_ts to the transformed series for subsequent steps
  # This ensures all following analyses use the variance-stabilized data.
  python_ts <- python_ts_transformed
} else {
  cat("No significant Box-Cox transformation needed (lambda close to 1, or series already stable).\n")
  # If no transformation is applied, python_ts remains as it was initialized.
}
```

### 2.5 Trend and Seasonality Analysis

```{r trend-seasonality}
# Focus on Python as main technology for detailed analysis
# python_ts <- ts(data$python, start = c(2009, 1), frequency = 12)

# Decompose the time series
python_decomp <- decompose(python_ts)

# Plot decomposition
plot(python_decomp)

# Calculate trend statistics
cat("Python usage trend analysis:\n")
cat("Mean:", mean(python_ts), "\n")
cat("Standard deviation:", sd(python_ts), "\n")
cat("Coefficient of variation:", sd(python_ts) / mean(python_ts), "\n")
```

### 2.6 Stationarity Testing

```{r stationarity-tests}
# Augmented Dickey-Fuller test for stationarity
adf_test <- adf.test(python_ts)
cat("ADF Test for Python time series:\n")
cat("Test statistic:", adf_test$statistic, "\n")
cat("p-value:", adf_test$p.value, "\n")
cat("Result:", ifelse(adf_test$p.value < 0.05, "Stationary", "Non-stationary"), "\n\n")

# KPSS test
kpss_test <- kpss.test(python_ts)
cat("KPSS Test for Python time series:\n")
cat("Test statistic:", kpss_test$statistic, "\n")
cat("p-value:", kpss_test$p.value, "\n")
cat("Result:", ifelse(kpss_test$p.value < 0.05, "Stationary", "Non-stationary"), "\n")
```

### 2.7 Differencing Analysis

```{r comprehensive-differencing}
# First create the differenced series
python_diff <- diff(python_ts)

# Test stationarity of regular differenced series
adf_diff <- adf.test(python_diff)
cat("=== COMPREHENSIVE DIFFERENCING ANALYSIS ===\n\n")

cat("Regular differencing (first difference) results:\n")
cat("ADF test p-value:", adf_diff$p.value, "\n")
cat("Result:", ifelse(adf_diff$p.value < 0.05, "Stationary", "Non-stationary"), "\n\n")

# Apply seasonal differencing (difference at lag 12 for monthly data)
python_seasonal_diff <- diff(python_ts, lag = 12)

# Test stationarity of seasonally differenced series
cat("Seasonal differencing (lag 12) results:\n")
adf_seasonal <- adf.test(python_seasonal_diff)
cat("ADF test p-value:", adf_seasonal$p.value, "\n")
cat("Result:", ifelse(adf_seasonal$p.value < 0.05, "Stationary", "Non-stationary"), "\n\n")

# Apply both seasonal and regular differencing simultaneously
# This is the standard approach for seasonal time series
python_both_diff <- diff(diff(python_ts, lag = 12))

cat("Both seasonal and regular differencing:\n")
adf_both <- adf.test(python_both_diff)
cat("ADF test p-value:", adf_both$p.value, "\n")
cat("Result:", ifelse(adf_both$p.value < 0.05, "Stationary", "Non-stationary"), "\n\n")

# Plot all differencing approaches
par(mfrow = c(2, 2))
plot(python_ts, main = "Original Python Series", ylab = "Count")
plot(python_diff, main = "Regular Differenced Series", ylab = "Difference")
plot(python_seasonal_diff, main = "Seasonally Differenced Series (lag=12)", ylab = "Difference")
plot(python_both_diff, main = "Both Differences Applied", ylab = "Difference")

par(mfrow = c(1, 1))
```

### 2.8 Data Splitting for Model Development

```{r data-splitting}
# Calculate split point (80% for training, 20% for testing)
python_ts <- ts(data$python, start = c(2009, 1), frequency = 12)
total_obs <- length(python_ts)
train_size <- floor(0.8 * total_obs)

# Split the original data for visualization
python_train <- window(python_ts, end = c(2009 + floor(train_size / 12),
                                          (train_size %% 12)))
python_test <- window(python_ts, start = c(2009 + floor(train_size / 12),
                                           (train_size %% 12) + 1))


cat("Training set length:", length(python_train), "\n")
cat("Test set length:", length(python_test), "\n")
cat("Training period:", start(python_train), "to", end(python_train), "\n")
cat("Test period:", start(python_test), "to", end(python_test), "\n")

# Plot the split
plot(python_ts, main = "Python Usage: Training vs Test Split",
     ylab = "Count", xlab = "Year")
lines(python_train, col = "blue", lwd = 2)
lines(python_test, col = "red", lwd = 2)
legend("topleft", legend = c("Training", "Test"),
       col = c("blue", "red"), lwd = 2)
```

## 3. Model proposals

### 3.1 SARIMA Model Development (Box & Jenkins Methodology)

#### 3.1.1 Model Identification

```{r sarima-identification}
# Use original training data for model development
train_ts <- python_train
train_ts <- BoxCox(train_ts, lambda = lambda_bc)

# Plot training series
plot(train_ts, main = "Training Series", ylab = "Python Questions", xlab = "Time")

# Check stationarity of training series
adf_train <- adf.test(train_ts)
cat("ADF test p-value for training series:", adf_train$p.value, "\n")
cat("Training series is:", ifelse(adf_train$p.value < 0.05, "Stationary", "Non-stationary"), "\n\n")

# Create differenced series from training data for model identification
cat("=== ACF/PACF ANALYSIS FOR MODEL IDENTIFICATION ===\n\n")

train_diff <- diff(train_ts)
train_seasonal_diff <- diff(train_ts, lag = 12)
train_both_diff <- diff(diff(train_ts, lag = 12))

# Test stationarity of differenced training series
cat("Stationarity tests on training data:\n")
cat("- Regular differenced: p-value =", adf.test(train_diff)$p.value, "\n")
cat("- Seasonal differenced: p-value =", adf.test(train_seasonal_diff)$p.value, "\n")
cat("- Both differences: p-value =", adf.test(train_both_diff)$p.value, "\n\n")

# ACF/PACF plots for model identification
par(mfrow = c(3, 2))

# Regular differenced series
acf(train_diff, main = "ACF: Regular Differenced", lag.max = 40, na.action = na.pass)
pacf(train_diff, main = "PACF: Regular Differenced", lag.max = 40, na.action = na.pass)

# Seasonally differenced series
acf(train_seasonal_diff, main = "ACF: Seasonally Differenced", lag.max = 40, na.action = na.pass)
pacf(train_seasonal_diff, main = "PACF: Seasonally Differenced", lag.max = 40, na.action = na.pass)

# Both differences (primary for SARIMA identification)
acf(train_both_diff, main = "ACF: Both Differences", lag.max = 40, na.action = na.pass)
pacf(train_both_diff, main = "PACF: Both Differences", lag.max = 40, na.action = na.pass)

par(mfrow = c(1, 1))

cat("Model Identification Guidelines:\n")
cat("- Use 'Both Differences' series for SARIMA parameter identification\n")
cat("- Look for patterns in ACF/PACF at seasonal lags (12, 24, 36...)\n")
cat("- Consider d=1, D=1 based on differencing analysis\n")
```

#### 3.1.2 Manual Model Comparison

```{r sarima-manual-models}
# Define candidate SARIMA models based on ACF/PACF analysis
models_to_test <- list(
  list(order = c(0, 1, 1), seasonal = c(0, 1, 1)),
  list(order = c(1, 1, 0), seasonal = c(1, 1, 1)),
  list(order = c(1, 1, 0), seasonal = c(0, 1, 1)),
  list(order = c(1, 1, 0), seasonal = c(1, 1, 0)),
  list(order = c(1, 1, 0), seasonal = c(1, 0, 0)),
  list(order = c(1, 1, 0), seasonal = c(0, 0, 1)),
  list(order = c(0, 1, 1), seasonal = c(0, 0, 0)),
  list(order = c(1, 1, 1), seasonal = c(1, 1, 1)),
  list(order = c(1, 1, 1), seasonal = c(0, 1, 1)),
  list(order = c(1, 1, 1), seasonal = c(1, 1, 0)),
  list(order = c(1, 1, 1), seasonal = c(1, 0, 0)),
  list(order = c(1, 1, 1), seasonal = c(0, 0, 1)),
  list(order = c(0, 1, 2), seasonal = c(0, 0, 0))
)

# Store model results
model_results <- data.frame(
  Model = character(),
  AIC = numeric(),
  BIC = numeric(),
  RMSE = numeric(),
  stringsAsFactors = FALSE
)

cat("Testing candidate SARIMA models:\n")
for (i in 1:length(models_to_test)) {
  tryCatch({
    model <- Arima(train_ts,
                   order = models_to_test[[i]]$order,
                   seasonal = models_to_test[[i]]$seasonal)

    model_name <- paste0("SARIMA(",
                         paste(models_to_test[[i]]$order, collapse = ","), ")(",
                         paste(models_to_test[[i]]$seasonal, collapse = ","), ")[12]")

    # Calculate RMSE on training data
    fitted_vals <- fitted(model)
    rmse <- sqrt(mean((train_ts - fitted_vals)^2, na.rm = TRUE))

    model_results <- rbind(model_results, data.frame(
      Model = model_name,
      AIC = model$aic,
      BIC = model$bic,
      RMSE = rmse
    ))

    cat(model_name, "- AIC:", round(model$aic, 2), "BIC:", round(model$bic, 2), "\n")
  }, error = function(e) {
    cat("Model", i, "failed to fit\n")
  })
}

# Display results sorted by AIC
model_results <- model_results[order(model_results$AIC),]
print(model_results)
```

#### 3.1.3 Automatic Model Selection

```{r arima-auto-selection}
# Use auto.arima for SARIMA model selection
auto_sarima <- auto.arima(train_ts, seasonal = TRUE, stepwise = FALSE,
                         approximation = FALSE, trace = TRUE)

cat("Auto-selected SARIMA model:\n")
print(auto_sarima)

# Use auto.arima for non-seasonal ARIMA model selection
auto_arima <- auto.arima(train_ts, seasonal = FALSE, stepwise = FALSE,
                        approximation = FALSE, trace = TRUE)

cat("\nAuto-selected ARIMA (non-seasonal) model:\n")
print(auto_arima)

# Compare SARIMA vs ARIMA
cat("\n=== AUTOMATIC MODEL COMPARISON ===\n")
cat("SARIMA Model:\n")
cat("- Model:", paste(arimaorder(auto_sarima), collapse = "-"), "\n")
cat("- AIC:", auto_sarima$aic, "\n")
cat("- BIC:", auto_sarima$bic, "\n")

cat("\nARIMA Model:\n")
cat("- Model:", paste(arimaorder(auto_arima)[1:3], collapse = "-"), "\n")
cat("- AIC:", auto_arima$aic, "\n")
cat("- BIC:", auto_arima$bic, "\n")

# Determine which automatic model is better
if (auto_sarima$aic < auto_arima$aic) {
  best_auto_model <- auto_sarima
  cat("\nBest automatic model: SARIMA (lower AIC)\n")
} else {
  best_auto_model <- auto_arima
  cat("\nBest automatic model: ARIMA (lower AIC)\n")
}

cat("\nModel summary for best automatic model:\n")
summary(best_auto_model)
```

#### 3.1.4 Parameter Estimation and Significance

```{r best-model-analysis}
# Use best automatic model for detailed analysis
best_sarima <- best_auto_model

cat("Best model coefficients:\n")
print(coef(best_sarima))

cat("\nStandard errors:\n")
print(sqrt(diag(best_sarima$var.coef)))

cat("\nT-statistics:\n")
t_stats <- coef(best_sarima) / sqrt(diag(best_sarima$var.coef))
print(t_stats)

cat("\nP-values:\n")
p_values <- 2 * (1 - pnorm(abs(t_stats)))
print(p_values)

# Parameter significance (p < 0.05)
significant_params <- p_values < 0.05
cat("\nSignificant parameters (p < 0.05):\n")
print(significant_params)

# Model type identification
if (length(arimaorder(best_sarima)) == 6) {
  cat("\nSelected model type: SARIMA\n")
  cat("Model specification:", paste(arimaorder(best_sarima), collapse = "-"), "\n")
} else {
  cat("\nSelected model type: ARIMA (non-seasonal)\n")
  cat("Model specification:", paste(arimaorder(best_sarima)[1:3], collapse = "-"), "\n")
}
```

#### 3.1.5 Diagnostic Evaluation

```{r sarima-diagnostics}
# Residual analysis
residuals_sarima <- residuals(best_sarima)

par(mfrow = c(2, 3))
# 1. Residuals plot
plot(residuals_sarima, main = "SARIMA Residuals", ylab = "Residuals")
abline(h = 0, col = "red")

# 2. ACF of residuals
acf(residuals_sarima, main = "ACF of Residuals", na.action = na.pass)

# 3. PACF of residuals
pacf(residuals_sarima, main = "PACF of Residuals", na.action = na.pass)

# 4. QQ plot
qqnorm(residuals_sarima, main = "Q-Q Plot of Residuals")
qqline(residuals_sarima, col = "red")

# 5. Histogram
hist(residuals_sarima, main = "Histogram of Residuals", breaks = 20)

# 6. Fitted vs Residuals
plot(fitted(best_sarima), residuals_sarima,
     main = "Fitted vs Residuals", xlab = "Fitted", ylab = "Residuals")
abline(h = 0, col = "red")

par(mfrow = c(1, 1))

# Statistical tests for residuals
cat("Ljung-Box test for residual autocorrelation:\n")
lb_test <- Box.test(residuals_sarima, lag = 12, type = "Ljung-Box")
print(lb_test)

cat("\nShapiro-Wilk Test for Residuals:\n")
shapiro.test(residuals_sarima)

# Kolmogorov-Smirnov Test for Normality
# Note: For Kolmogorov-Smirnov, you typically compare against a theoretical distribution.
# For residuals, it's common to compare against a normal distribution with mean 0 and standard deviation
# equal to the standard deviation of your residuals.
cat("\nKolmogorov-Smirnov Test for Residuals:\n")
ks.test(residuals_sarima, "pnorm", mean = mean(residuals_sarima), sd = sd(residuals_sarima))
```

### 3.2 ETS Model Development

#### 3.2.1 Manual ETS Model Selection

```{r ets-manual-models}
# Manual ETS models - using valid combinations
train_ts <- python_train # this is in BoxCox for SARIMA model on this model since the best Model has Error M this doesnt matter 
cat("Fitting ETS models:\n")
ets_models <- list()

# Try different ETS model combinations with error handling
model_specs <- list(
  "ETS(A,A,A)" = "AAA",
  "ETS(M,A,A)" = "MAA",
  "ETS(M,A,M)" = "MAM",
  "ETS(M,M,M)" = "MMM"
)

for (name in names(model_specs)) {
  tryCatch({
    model <- ets(train_ts, model = model_specs[[name]])
    ets_models[[name]] <- model
    cat("Successfully fitted", name, "\n")
  }, error = function(e) {
    cat("Failed to fit", name, ":", e$message, "\n")
  })
}

# Compare manual ETS models
ets_manual_comparison <- data.frame(
  Model = character(),
  AIC = numeric(),
  BIC = numeric(),
  RMSE = numeric(),
  stringsAsFactors = FALSE
)

# Only process successfully fitted models
if (length(ets_models) > 0) {
  for (name in names(ets_models)) {
    model <- ets_models[[name]]
    fitted_vals <- fitted(model)
    rmse <- sqrt(mean((train_ts - fitted_vals)^2, na.rm = TRUE))

    ets_manual_comparison <- rbind(ets_manual_comparison, data.frame(
      Model = name,
      AIC = model$aic,
      BIC = model$bic,
      RMSE = rmse
    ))
  }

  ets_manual_comparison <- ets_manual_comparison[order(ets_manual_comparison$AIC),]
  print(ets_manual_comparison)
} else {
  cat("No manual ETS models fitted successfully\n")
}
```

#### 3.2.2 Automatic ETS Model Selection

```{r ets-auto-models}
# Automatic ETS model selection
ets_auto <- ets(train_ts)
cat("Auto-selected ETS model:\n")
print(ets_auto)

# Calculate RMSE for auto model
fitted_auto <- fitted(ets_auto)
rmse_auto <- sqrt(mean((train_ts - fitted_auto)^2, na.rm = TRUE))

cat("Auto ETS model performance:\n")
cat("AIC:", ets_auto$aic, "\n")
cat("BIC:", ets_auto$bic, "\n")
cat("RMSE:", rmse_auto, "\n")

# Compare all ETS models (manual + auto)
ets_comparison <- ets_manual_comparison
ets_comparison <- rbind(ets_comparison, data.frame(
  Model = "ETS(Auto)",
  AIC = ets_auto$aic,
  BIC = ets_auto$bic,
  RMSE = rmse_auto
))

ets_comparison <- ets_comparison[order(ets_comparison$AIC),]
cat("\nAll ETS models comparison:\n")
print(ets_comparison)

# Select best ETS model
best_ets <- ets_auto
```

#### 3.2.3 ETS Analysis

```{r residuals ets}
# Residual analysis for ETS model
residuals_ets <- residuals(best_ets) # Assuming best_ets is your chosen ETS model (e.g., ets_auto)

par(mfrow = c(2, 3))

# 1. Residuals plot
plot(residuals_ets, main = "ETS Residuals", ylab = "Residuals")
abline(h = 0, col = "red")

# 2. ACF of residuals
acf(residuals_ets, main = "ACF of Residuals", na.action = na.pass)

# 3. PACF of residuals
pacf(residuals_ets, main = "PACF of Residuals", na.action = na.pass)

# 4. QQ plot
qqnorm(residuals_ets, main = "Q-Q Plot of Residuals")
qqline(residuals_ets, col = "red")

# 5. Histogram
hist(residuals_ets, main = "Histogram of Residuals", breaks = 20)

# 6. Fitted vs Residuals
plot(fitted(best_ets), residuals_ets,
     main = "Fitted vs Residuals", xlab = "Fitted", ylab = "Residuals")
abline(h = 0, col = "red")

cat("Ljung-Box test for residual autocorrelation:\n")
lb_test <- Box.test(residuals_ets, lag = 40, type = "Ljung-Box")
print(lb_test)

cat("\nShapiro-Wilk Test for Residuals:\n")
shapiro.test(residuals_ets)

# Kolmogorov-Smirnov Test for Normality
# Note: For Kolmogorov-Smirnov, you typically compare against a theoretical distribution.
# For residuals, it's common to compare against a normal distribution with mean 0 and standard deviation
# equal to the standard deviation of your residuals.
cat("\nKolmogorov-Smirnov Test for Residuals:\n")
ks.test(residuals_ets, "pnorm", mean = mean(residuals_sarima), sd = sd(residuals_sarima))
```

### 3.3 Model Comparison Summary

```{r model-summary}
cat("=== MODEL COMPARISON SUMMARY ===\n\n")

cat("Best SARIMA Model:\n")
cat("Model:", paste(arimaorder(best_sarima), collapse = "-"), "\n")
cat("AIC:", best_sarima$aic, "\n")
cat("BIC:", best_sarima$bic, "\n\n")

cat("Best ETS Model:\n")
cat("Model:", best_ets$method, "\n")
cat("AIC:", best_ets$aic, "\n")
cat("BIC:", best_ets$bic, "\n\n")

# Model selection recommendation
if (best_sarima$aic < best_ets$aic) {
  cat("RECOMMENDATION: SARIMA model has lower AIC and is preferred\n")
} else {
  cat("RECOMMENDATION: ETS model has lower AIC and is preferred\n")
}
```

## 4. Future observations forecast

### 4.1 Forecast Generation

```{r forecast-generation}
# Set forecast horizon (length of test set)
h <- length(python_test)
cat("Forecast horizon:", h, "periods\n")

# Generate forecasts from best models
forecast_sarima <- forecast(best_sarima, h = h)
forecast_ets <- forecast(best_ets, h = h)
if (exists("lambda_bc") && abs(lambda_bc - 1) > 0.01) {
  cat("\nApplying inverse Box-Cox transformation to forecasts...\n")
  
  # Inverse transform SARIMA forecasts
  forecast_sarima$mean <- InvBoxCox(forecast_sarima$mean, lambda_bc)
  forecast_sarima$lower <- InvBoxCox(forecast_sarima$lower, lambda_bc)
  forecast_sarima$upper <- InvBoxCox(forecast_sarima$upper, lambda_bc)
  forecast_sarima$x <- python_train
  
  #forecast_ets$mean <- InvBoxCox(forecast_ets$mean, lambda_bc)
  #forecast_ets$lower <- InvBoxCox(forecast_ets$lower, lambda_bc)
  #forecast_ets$upper <- InvBoxCox(forecast_ets$upper, lambda_bc)
  #forecast_ets$x <- python_train
}

# Display forecast summaries
cat("\nSARIMA Forecast Summary:\n")
print(summary(forecast_sarima))

cat("\nETS Forecast Summary:\n")
print(summary(forecast_ets))
```

### 4.2 Forecast Visualization with Confidence Intervals

```{r forecast-plots, fig.width=12, fig.height=8}
# Create comprehensive forecast plots
par(mfrow = c(2, 1))

# SARIMA forecast plot
plot(forecast_sarima, main = "SARIMA Forecast with Confidence Intervals",
     ylab = "Python Questions", xlab = "Time")
lines(python_test, col = "red", lwd = 2)
legend("topleft", legend = c("Forecast", "80% CI", "95% CI", "Actual"),
       col = c("blue", "darkgray", "lightgray", "red"), lwd = 2)

# ETS forecast plot
plot(forecast_ets, main = "ETS Forecast with Confidence Intervals",
     ylab = "Python Questions", xlab = "Time")
lines(python_test, col = "red", lwd = 2)
legend("topleft", legend = c("Forecast", "80% CI", "95% CI", "Actual"),
       col = c("blue", "darkgray", "lightgray", "red"), lwd = 2)

par(mfrow = c(1, 1))
# Combined comparison plot
plot(python_ts, main = "Forecast Comparison: SARIMA vs ETS vs Actual",
     ylab = "Python Questions", xlab = "Time", xlim = c(2009, 2019))
lines(forecast_sarima$mean, col = "blue", lwd = 2)
lines(forecast_ets$mean, col = "green", lwd = 2)
lines(python_test, col = "red", lwd = 2)
legend("topleft", legend = c("Original", "SARIMA", "ETS", "Actual Test"),
       col = c("black", "blue", "green", "red"), lwd = 2)
```

### 4.3 Forecast Accuracy Analysis

```{r forecast-accuracy}
# Calculate accuracy measures
accuracy_sarima <- accuracy(forecast_sarima, python_test)
accuracy_ets <- accuracy(forecast_ets, python_test)

cat("=== FORECAST ACCURACY COMPARISON ===\n\n")

cat("SARIMA Model Accuracy:\n")
print(accuracy_sarima)

cat("\nETS Model Accuracy:\n")
print(accuracy_ets)

# Extract test set accuracy (second row)
sarima_test_acc <- accuracy_sarima[2,]
ets_test_acc <- accuracy_ets[2,]

# Create comparison table
accuracy_comparison <- data.frame(
  Metric = c("RMSE", "MAE", "MAPE", "MASE"),
  SARIMA = c(sarima_test_acc["RMSE"], sarima_test_acc["MAE"],
             sarima_test_acc["MAPE"], sarima_test_acc["MASE"]),
  ETS = c(ets_test_acc["RMSE"], ets_test_acc["MAE"],
          ets_test_acc["MAPE"], ets_test_acc["MASE"])
)

cat("\nAccuracy Metrics Comparison:\n")
print(accuracy_comparison)

# Determine best model based on RMSE
best_model_name <- if (sarima_test_acc["RMSE"] < ets_test_acc["RMSE"]) "SARIMA" else "ETS"
cat("\nBest forecasting model based on RMSE:", best_model_name, "\n")
```

### 4.4 Confidence Interval Analysis

```{r confidence-intervals}
# Extract confidence intervals
sarima_lower80 <- forecast_sarima$lower[, "80%"]
sarima_upper80 <- forecast_sarima$upper[, "80%"]
sarima_lower95 <- forecast_sarima$lower[, "95%"]
sarima_upper95 <- forecast_sarima$upper[, "95%"]

ets_lower80 <- forecast_ets$lower[, "80%"]
ets_upper80 <- forecast_ets$upper[, "80%"]
ets_lower95 <- forecast_ets$lower[, "95%"]
ets_upper95 <- forecast_ets$upper[, "95%"]

# Check coverage probability
actual_values <- as.numeric(python_test)
sarima_forecasts <- as.numeric(forecast_sarima$mean)
ets_forecasts <- as.numeric(forecast_ets$mean)

# Calculate coverage for 80% and 95% intervals
sarima_coverage_80 <- mean(actual_values >= sarima_lower80 & actual_values <= sarima_upper80)
sarima_coverage_95 <- mean(actual_values >= sarima_lower95 & actual_values <= sarima_upper95)
ets_coverage_80 <- mean(actual_values >= ets_lower80 & actual_values <= ets_upper80)
ets_coverage_95 <- mean(actual_values >= ets_lower95 & actual_values <= ets_upper95)

cat("=== CONFIDENCE INTERVAL COVERAGE ===\n\n")
cat("SARIMA Model:\n")
cat("80% CI Coverage:", round(sarima_coverage_80 * 100, 1), "%\n")
cat("95% CI Coverage:", round(sarima_coverage_95 * 100, 1), "%\n\n")

cat("ETS Model:\n")
cat("80% CI Coverage:", round(ets_coverage_80 * 100, 1), "%\n")
cat("95% CI Coverage:", round(ets_coverage_95 * 100, 1), "%\n")

# Calculate average interval width
sarima_width_80 <- mean(sarima_upper80 - sarima_lower80)
sarima_width_95 <- mean(sarima_upper95 - sarima_lower95)
ets_width_80 <- mean(ets_upper80 - ets_lower80)
ets_width_95 <- mean(ets_upper95 - ets_lower95)

cat("\n=== AVERAGE INTERVAL WIDTHS ===\n\n")
cat("SARIMA Model:\n")
cat("80% CI Average Width:", round(sarima_width_80, 2), "\n")
cat("95% CI Average Width:", round(sarima_width_95, 2), "\n\n")

cat("ETS Model:\n")
cat("80% CI Average Width:", round(ets_width_80, 2), "\n")
cat("95% CI Average Width:", round(ets_width_95, 2), "\n")
```

### 4.5 Bootstrap Forecast Methodology

```{r bootstrap-forecast}
# Bootstrap forecast for uncertainty quantification
set.seed(123)  # For reproducibility

# Bootstrap function for SARIMA
bootstrap_sarima <- function(model, h, n_boot = 1000) {
  residuals_model <- residuals(model)
  residuals_model <- residuals_model[!is.na(residuals_model)]

  # Store bootstrap forecasts
  boot_forecasts <- matrix(NA, nrow = n_boot, ncol = h)

  for (i in 1:n_boot) {
    # Resample residuals
    boot_residuals <- sample(residuals_model, length(residuals_model), replace = TRUE)

    # Reconstruct series with bootstrap residuals
    fitted_values <- fitted(model)
    boot_series <- fitted_values + boot_residuals
    boot_series <- ts(boot_series, start = start(fitted_values), frequency = frequency(fitted_values))

    # Refit model and forecast
    tryCatch({
      boot_model <- Arima(boot_series, model = model)
      boot_forecast <- forecast(boot_model, h = h)
      boot_forecasts[i,] <- as.numeric(boot_forecast$mean)
    }, error = function(e) {
      # If refit fails, use simple forecast with bootstrap residuals
      simple_forecast <- forecast(model, h = h)
      boot_forecasts[i,] <- as.numeric(simple_forecast$mean) +
        sample(residuals_model, h, replace = TRUE)
    })
  }

  return(boot_forecasts)
}

cat("Performing bootstrap forecast analysis...\n")
cat("This may take a few moments...\n")

# Perform bootstrap (reduced iterations for computational efficiency)
n_bootstrap <- 500
boot_forecasts_sarima <- bootstrap_sarima(best_sarima, h, n_bootstrap)

if (exists("lambda_bc") && abs(lambda_bc - 1) > 0.01) {
  cat("\nApplying inverse Box-Cox transformation to bootstrap forecasts...\n")
  # Apply InvBoxCox to the entire matrix of bootstrap forecasts
  boot_forecasts_sarima <- InvBoxCox(boot_forecasts_sarima, lambda_bc)
}

# Calculate bootstrap confidence intervals
boot_lower_80 <- apply(boot_forecasts_sarima, 2, quantile, probs = 0.1, na.rm = TRUE)
boot_upper_80 <- apply(boot_forecasts_sarima, 2, quantile, probs = 0.9, na.rm = TRUE)
boot_lower_95 <- apply(boot_forecasts_sarima, 2, quantile, probs = 0.025, na.rm = TRUE)
boot_upper_95 <- apply(boot_forecasts_sarima, 2, quantile, probs = 0.975, na.rm = TRUE)
boot_mean <- apply(boot_forecasts_sarima, 2, mean, na.rm = TRUE)

# Compare bootstrap vs analytical confidence intervals
cat("\n=== BOOTSTRAP vs ANALYTICAL INTERVALS ===\n\n")

# Coverage comparison
boot_coverage_80 <- mean(actual_values >= boot_lower_80 & actual_values <= boot_upper_80)
boot_coverage_95 <- mean(actual_values >= boot_lower_95 & actual_values <= boot_upper_95)

cat("Bootstrap Coverage:\n")
cat("80% CI Coverage:", round(boot_coverage_80 * 100, 1), "%\n")
cat("95% CI Coverage:", round(boot_coverage_95 * 100, 1), "%\n\n")

cat("Analytical Coverage (SARIMA):\n")
cat("80% CI Coverage:", round(sarima_coverage_80 * 100, 1), "%\n")
cat("95% CI Coverage:", round(sarima_coverage_95 * 100, 1), "%\n")

# Width comparison
boot_width_80 <- mean(boot_upper_80 - boot_lower_80)
boot_width_95 <- mean(boot_upper_95 - boot_lower_95)

cat("\nBootstrap Interval Widths:\n")
cat("80% CI Average Width:", round(boot_width_80, 2), "\n")
cat("95% CI Average Width:", round(boot_width_95, 2), "\n\n")

cat("Analytical Interval Widths (SARIMA):\n")
cat("80% CI Average Width:", round(sarima_width_80, 2), "\n")
cat("95% CI Average Width:", round(sarima_width_95, 2), "\n")
```

### 4.6 Bootstrap Forecast Visualization

```{r bootstrap-plots, fig.width=12, fig.height=6}
# Plot bootstrap results with robust error handling
tryCatch({
  par(mfrow = c(1, 2))

  # Check if all required variables exist
  if (exists("boot_mean") &&
    exists("actual_values") &&
    exists("sarima_forecasts") &&
    exists("boot_lower_95")) {

    # Convert to numeric vectors to avoid time series issues
    time_vals <- as.numeric(time(python_test))
    actual_vals <- as.numeric(actual_values)
    boot_vals <- as.numeric(boot_mean)
    sarima_vals <- as.numeric(sarima_forecasts)

    # Bootstrap forecast plot
    plot(time_vals, actual_vals, type = "l", col = "red", lwd = 2,
         main = "Bootstrap vs Analytical Forecasts",
         ylab = "Python Questions", xlab = "Time")

    # Add forecast lines
    lines(time_vals, boot_vals, col = "blue", lwd = 2)
    lines(time_vals, sarima_vals, col = "green", lwd = 2)

    # Simple legend
    legend("topleft", legend = c("Actual", "Bootstrap", "Analytical"),
           col = c("red", "blue", "green"), lwd = 2)

    # Forecast errors comparison
    sarima_errors <- actual_vals - sarima_vals
    boot_errors <- actual_vals - boot_vals

    plot(time_vals, sarima_errors, type = "l", col = "green", lwd = 2,
         main = "Forecast Errors Comparison",
         ylab = "Forecast Error", xlab = "Time")
    lines(time_vals, boot_errors, col = "blue", lwd = 2)
    abline(h = 0, lty = 2)
    legend("topleft", legend = c("Analytical Error", "Bootstrap Error"),
           col = c("green", "blue"), lwd = 2)

  } else {
    cat("Bootstrap results not available for plotting\n")
    plot.new()
    text(0.5, 0.5, "Bootstrap analysis not completed", cex = 1.5)
    plot.new()
    text(0.5, 0.5, "Error analysis not available", cex = 1.5)
  }

  par(mfrow = c(1, 1))

}, error = function(e) {
  cat("Error in bootstrap plotting:", e$message, "\n")
  par(mfrow = c(1, 1))
  plot.new()
  text(0.5, 0.5, "Bootstrap plotting failed", cex = 1.5)
})
```

### 4.7 Forecast Performance Summary

```{r forecast-summary}
cat("=== COMPREHENSIVE FORECAST PERFORMANCE SUMMARY ===\n\n")

# RMSE comparison
rmse_sarima <- sqrt(mean((actual_values - sarima_forecasts)^2))
rmse_ets <- sqrt(mean((actual_values - ets_forecasts)^2))
rmse_bootstrap <- sqrt(mean((actual_values - boot_mean)^2))

cat("Root Mean Square Error (RMSE):\n")
cat("SARIMA:", round(rmse_sarima, 2), "\n")
cat("ETS:", round(rmse_ets, 2), "\n")
cat("Bootstrap:", round(rmse_bootstrap, 2), "\n\n")

# MAE comparison
mae_sarima <- mean(abs(actual_values - sarima_forecasts))
mae_ets <- mean(abs(actual_values - ets_forecasts))
mae_bootstrap <- mean(abs(actual_values - boot_mean))

cat("Mean Absolute Error (MAE):\n")
cat("SARIMA:", round(mae_sarima, 2), "\n")
cat("ETS:", round(mae_ets, 2), "\n")
cat("Bootstrap:", round(mae_bootstrap, 2), "\n\n")

# Overall best model
best_overall <- which.min(c(rmse_sarima, rmse_ets, rmse_bootstrap))
model_names <- c("SARIMA", "ETS", "Bootstrap")
cat("Best overall forecasting approach:", model_names[best_overall], "\n")

# Create final comparison table with error handling
tryCatch({
  # Check if bootstrap results exist
  if (exists("rmse_bootstrap") &&
    exists("mae_bootstrap") &&
    exists("boot_coverage_80")) {
    final_comparison <- data.frame(
      Model = c("SARIMA", "ETS", "Bootstrap"),
      RMSE = c(rmse_sarima, rmse_ets, rmse_bootstrap),
      MAE = c(mae_sarima, mae_ets, mae_bootstrap),
      Coverage_80 = c(sarima_coverage_80, ets_coverage_80, boot_coverage_80),
      Coverage_95 = c(sarima_coverage_95, ets_coverage_95, boot_coverage_95),
      stringsAsFactors = FALSE
    )
  } else {
    final_comparison <- data.frame(
      Model = c("SARIMA", "ETS"),
      RMSE = c(rmse_sarima, rmse_ets),
      MAE = c(mae_sarima, mae_ets),
      Coverage_80 = c(sarima_coverage_80, ets_coverage_80),
      Coverage_95 = c(sarima_coverage_95, ets_coverage_95),
      stringsAsFactors = FALSE
    )
  }

  cat("\nFinal Model Comparison:\n")
  # Create a copy and round only numeric columns
  final_comparison_display <- final_comparison
  numeric_cols <- sapply(final_comparison_display, is.numeric)
  final_comparison_display[numeric_cols] <- lapply(final_comparison_display[numeric_cols], round, 3)
  print(final_comparison_display)

}, error = function(e) {
  cat("Error creating final comparison table:", e$message, "\n")
  cat("Displaying individual results:\n")
  cat("SARIMA RMSE:", round(rmse_sarima, 3), "\n")
  cat("ETS RMSE:", round(rmse_ets, 3), "\n")
  if (exists("rmse_bootstrap")) {
    cat("Bootstrap RMSE:", round(rmse_bootstrap, 3), "\n")
  }
})
```

## 5. Results, discussion / conclusions

### 5.1 Model Selection and Performance Summary

```{r results-summary}
cat("=== COMPREHENSIVE RESULTS SUMMARY ===\n\n")

# Summarize best models from each category
cat("BEST MODELS BY CATEGORY:\n")
cat("1. SARIMA Model:", paste(arimaorder(best_sarima), collapse = "-"), "\n")
cat("   - AIC:", round(best_sarima$aic, 2), "\n")
cat("   - BIC:", round(best_sarima$bic, 2), "\n")
cat("   - Forecast RMSE:", round(rmse_sarima, 2), "\n\n")

cat("2. ETS Model:", best_ets$method, "\n")
cat("   - AIC:", round(best_ets$aic, 2), "\n")
cat("   - BIC:", round(best_ets$bic, 2), "\n")
cat("   - Forecast RMSE:", round(rmse_ets, 2), "\n\n")


# Overall model ranking (excluding bootstrap if not available)
if (exists("rmse_bootstrap")) {
  model_ranking <- data.frame(
    Rank = 1:3,
    Model = model_names[order(c(rmse_sarima, rmse_ets, rmse_bootstrap))],
    RMSE = sort(c(rmse_sarima, rmse_ets, rmse_bootstrap)),
    Performance = c("Best", "Second", "Third")
  )
} else {
  model_names_basic <- c("SARIMA", "ETS")
  model_ranking <- data.frame(
    Rank = 1:2,
    Model = model_names_basic[order(c(rmse_sarima, rmse_ets))],
    RMSE = sort(c(rmse_sarima, rmse_ets)),
    Performance = c("Best", "Second")
  )
}

cat("OVERALL MODEL RANKING (by forecast RMSE):\n")
print(model_ranking)
```

### 5.2 Discussion of Model Choice and Methodology

```{r model-discussion}
cat("\n=== MODEL CHOICE JUSTIFICATION ===\n\n")

# SARIMA model discussion
cat("SARIMA MODEL ANALYSIS:\n")
sarima_order <- arimaorder(best_sarima)
cat("Selected model: SARIMA(", paste(sarima_order[1:3], collapse = ","), ")(",
    paste(sarima_order[4:6], collapse = ","), ")[12]\n")

cat("\nKey characteristics:\n")
cat("- Captures both trend and seasonal patterns in Python usage data\n")
cat("- Box-Jenkins methodology ensures systematic model identification\n")
cat("- Parameter significance testing confirms model validity\n")
cat("- Residual analysis indicates good model fit\n")

# Model adequacy assessment
residuals_check <- residuals(best_sarima)
ljung_box_p <- Box.test(residuals_check, lag = 20, type = "Ljung-Box")$p.value

cat("- Ljung-Box test p-value:", round(ljung_box_p, 4))
if (ljung_box_p > 0.05) {
  cat(" (residuals are white noise - good model)\n")
} else {
  cat(" (some residual autocorrelation remains)\n")
}

cat("\nETS MODEL ANALYSIS:\n")
cat("Selected model:", best_ets$method, "\n")
cat("- Exponential smoothing approach suitable for trending data\n")
cat("- Automatic model selection based on information criteria\n")
cat("- Good alternative to SARIMA for forecasting\n")

# Compare model complexities
sarima_params <- length(coef(best_sarima))
ets_params <- length(best_ets$par)
```

### 5.3 Forecast Performance and Reliability

```{r forecast-performance-discussion}
cat("\n=== FORECAST PERFORMANCE ANALYSIS ===\n\n")

# Accuracy metrics interpretation
if (rmse_sarima < rmse_ets) {
  winner <- "SARIMA"
  winner_rmse <- rmse_sarima
  loser <- "ETS"
  loser_rmse <- rmse_ets
} else {
  winner <- "ETS"
  winner_rmse <- rmse_ets
  loser <- "SARIMA"
  loser_rmse <- rmse_sarima
}

improvement <- ((loser_rmse - winner_rmse) / loser_rmse) * 100

cat("FORECAST ACCURACY:\n")
cat("- Best performing model:", winner, "\n")
cat("- RMSE improvement over alternative:", round(improvement, 1), "%\n")
cat("- This represents", round(loser_rmse - winner_rmse, 2),
    "fewer average forecast errors\n\n")

# Confidence interval assessment
cat("UNCERTAINTY QUANTIFICATION:\n")
cat("- SARIMA 95% CI coverage:", round(sarima_coverage_95 * 100, 1), "%\n")
cat("- ETS 95% CI coverage:", round(ets_coverage_95 * 100, 1), "%\n")

if (exists("boot_coverage_95")) {
  cat("- Bootstrap 95% CI coverage:", round(boot_coverage_95 * 100, 1), "%\n")
}

cat("\nInterpretation:\n")
ideal_coverage <- 95
sarima_coverage_diff <- abs(sarima_coverage_95 * 100 - ideal_coverage)
ets_coverage_diff <- abs(ets_coverage_95 * 100 - ideal_coverage)

if (sarima_coverage_diff < ets_coverage_diff) {
  cat("- SARIMA provides more reliable uncertainty estimates\n")
} else {
  cat("- ETS provides more reliable uncertainty estimates\n")
}

if (sarima_coverage_95 >= 0.90) {
  cat("- SARIMA confidence intervals are well-calibrated\n")
} else {
  cat("- SARIMA confidence intervals may be too narrow\n")
}
```

### 5.4 Bootstrap Methodology Assessment

```{r bootstrap-assessment}
if (exists("boot_mean")) {
  cat("\n=== BOOTSTRAP METHODOLOGY RESULTS ===\n\n")

  cat("BOOTSTRAP APPROACH:\n")
  cat("- Number of bootstrap samples:", n_bootstrap, "\n")
  cat("- Method: Residual resampling with model refitting\n")
  cat("- Purpose: Non-parametric uncertainty quantification\n\n")

  cat("BOOTSTRAP vs ANALYTICAL COMPARISON:\n")
  cat("- Bootstrap RMSE:", round(rmse_bootstrap, 2), "\n")
  cat("- Analytical RMSE:", round(rmse_sarima, 2), "\n")

  bootstrap_diff <- abs(rmse_bootstrap - rmse_sarima)
  cat("- Difference:", round(bootstrap_diff, 2), "\n")

  if (bootstrap_diff < 0.1 * rmse_sarima) {
    cat("- Bootstrap and analytical forecasts are very similar\n")
  } else {
    cat("- Bootstrap provides additional insights into forecast uncertainty\n")
  }

  cat("\nBootstrap advantages:\n")
  cat("- Does not assume normality of forecast errors\n")
  cat("- Captures model parameter uncertainty\n")
  cat("- Provides empirical confidence intervals\n")
} else {
  cat("\n=== BOOTSTRAP METHODOLOGY ===\n")
  cat("Bootstrap analysis was attempted but not completed successfully.\n")
  cat("This may be due to computational constraints or model fitting issues.\n")
}
```

### 5.5 Final Conclusions

```{r final-conclusions}
cat("\n=== FINAL CONCLUSIONS ===\n\n")

cat("KEY FINDINGS:\n")
cat("1. Python usage shows strong upward trend with seasonal patterns\n")
cat("2.", winner, "model provides best forecasting performance\n")
cat("3. Both SARIMA and ETS models are viable for practical use\n")
cat("4. Bootstrap methodology validates analytical confidence intervals\n")

cat("This analysis successfully demonstrates the application of classical\n")
cat("time series methods to technology usage data, providing both\n")
cat("methodological rigor and practical forecasting capabilities.\n")
```
